{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import os\n",
    "\n",
    "# Set the directory path where your files are located\n",
    "directory = r\"C:\\Users\\Henri\\Documents\\GitHub\\Predi_Conso_Elec_Region\\Predi_Conso_Elec_Region\\notebooks\\Region tests\\HDF\"\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if \"CVL\" in filename:\n",
    "        new_filename = filename.replace(\"CVL\", \"HDF\")\n",
    "        old_path = os.path.join(directory, filename)\n",
    "        new_path = os.path.join(directory, new_filename)\n",
    "        os.rename(old_path, new_path)\n",
    "        print(f\"Renamed: {filename} ‚ûù {new_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied and renamed: dictionaries.py ‚ûù IDF\n",
      "Copied and renamed: M12_HDF_03-05_20D0.ipynb ‚ûù M12_IDF_03-05_20D0.ipynb in IDF\n",
      "Copied and renamed: M12_HDF_training.ipynb ‚ûù M12_IDF_training.ipynb in IDF\n",
      "Copied and renamed: M18_HDF_03-05_14D0.ipynb ‚ûù M18_IDF_03-05_14D0.ipynb in IDF\n",
      "Copied and renamed: M18_HDF_training.ipynb ‚ûù M18_IDF_training.ipynb in IDF\n",
      "Copied and renamed: M24_HDF_03-05_08D0.ipynb ‚ûù M24_IDF_03-05_08D0.ipynb in IDF\n",
      "Copied and renamed: M24_HDF_training.ipynb ‚ûù M24_IDF_training.ipynb in IDF\n",
      "Copied and renamed: M36_HDF_03-05_02D0.ipynb ‚ûù M36_IDF_03-05_02D0.ipynb in IDF\n",
      "Copied and renamed: M36_HDF_training.ipynb ‚ûù M36_IDF_training.ipynb in IDF\n",
      "Copied and renamed: M48_HDF_03-05_02D0.ipynb ‚ûù M48_IDF_03-05_02D0.ipynb in IDF\n",
      "Copied and renamed: M48_HDF_training.ipynb ‚ûù M48_IDF_training.ipynb in IDF\n",
      "Copied and renamed: pipeline_cons_temp_df.ipynb ‚ûù IDF\n",
      "Copied and renamed: dictionaries.py ‚ûù NOR\n",
      "Copied and renamed: M12_HDF_03-05_20D0.ipynb ‚ûù M12_NOR_03-05_20D0.ipynb in NOR\n",
      "Copied and renamed: M12_HDF_training.ipynb ‚ûù M12_NOR_training.ipynb in NOR\n",
      "Copied and renamed: M18_HDF_03-05_14D0.ipynb ‚ûù M18_NOR_03-05_14D0.ipynb in NOR\n",
      "Copied and renamed: M18_HDF_training.ipynb ‚ûù M18_NOR_training.ipynb in NOR\n",
      "Copied and renamed: M24_HDF_03-05_08D0.ipynb ‚ûù M24_NOR_03-05_08D0.ipynb in NOR\n",
      "Copied and renamed: M24_HDF_training.ipynb ‚ûù M24_NOR_training.ipynb in NOR\n",
      "Copied and renamed: M36_HDF_03-05_02D0.ipynb ‚ûù M36_NOR_03-05_02D0.ipynb in NOR\n",
      "Copied and renamed: M36_HDF_training.ipynb ‚ûù M36_NOR_training.ipynb in NOR\n",
      "Copied and renamed: M48_HDF_03-05_02D0.ipynb ‚ûù M48_NOR_03-05_02D0.ipynb in NOR\n",
      "Copied and renamed: M48_HDF_training.ipynb ‚ûù M48_NOR_training.ipynb in NOR\n",
      "Copied and renamed: pipeline_cons_temp_df.ipynb ‚ûù NOR\n",
      "Copied and renamed: dictionaries.py ‚ûù OCC\n",
      "Copied and renamed: M12_HDF_03-05_20D0.ipynb ‚ûù M12_OCC_03-05_20D0.ipynb in OCC\n",
      "Copied and renamed: M12_HDF_training.ipynb ‚ûù M12_OCC_training.ipynb in OCC\n",
      "Copied and renamed: M18_HDF_03-05_14D0.ipynb ‚ûù M18_OCC_03-05_14D0.ipynb in OCC\n",
      "Copied and renamed: M18_HDF_training.ipynb ‚ûù M18_OCC_training.ipynb in OCC\n",
      "Copied and renamed: M24_HDF_03-05_08D0.ipynb ‚ûù M24_OCC_03-05_08D0.ipynb in OCC\n",
      "Copied and renamed: M24_HDF_training.ipynb ‚ûù M24_OCC_training.ipynb in OCC\n",
      "Copied and renamed: M36_HDF_03-05_02D0.ipynb ‚ûù M36_OCC_03-05_02D0.ipynb in OCC\n",
      "Copied and renamed: M36_HDF_training.ipynb ‚ûù M36_OCC_training.ipynb in OCC\n",
      "Copied and renamed: M48_HDF_03-05_02D0.ipynb ‚ûù M48_OCC_03-05_02D0.ipynb in OCC\n",
      "Copied and renamed: M48_HDF_training.ipynb ‚ûù M48_OCC_training.ipynb in OCC\n",
      "Copied and renamed: pipeline_cons_temp_df.ipynb ‚ûù OCC\n",
      "Copied and renamed: dictionaries.py ‚ûù PAC\n",
      "Copied and renamed: M12_HDF_03-05_20D0.ipynb ‚ûù M12_PAC_03-05_20D0.ipynb in PAC\n",
      "Copied and renamed: M12_HDF_training.ipynb ‚ûù M12_PAC_training.ipynb in PAC\n",
      "Copied and renamed: M18_HDF_03-05_14D0.ipynb ‚ûù M18_PAC_03-05_14D0.ipynb in PAC\n",
      "Copied and renamed: M18_HDF_training.ipynb ‚ûù M18_PAC_training.ipynb in PAC\n",
      "Copied and renamed: M24_HDF_03-05_08D0.ipynb ‚ûù M24_PAC_03-05_08D0.ipynb in PAC\n",
      "Copied and renamed: M24_HDF_training.ipynb ‚ûù M24_PAC_training.ipynb in PAC\n",
      "Copied and renamed: M36_HDF_03-05_02D0.ipynb ‚ûù M36_PAC_03-05_02D0.ipynb in PAC\n",
      "Copied and renamed: M36_HDF_training.ipynb ‚ûù M36_PAC_training.ipynb in PAC\n",
      "Copied and renamed: M48_HDF_03-05_02D0.ipynb ‚ûù M48_PAC_03-05_02D0.ipynb in PAC\n",
      "Copied and renamed: M48_HDF_training.ipynb ‚ûù M48_PAC_training.ipynb in PAC\n",
      "Copied and renamed: pipeline_cons_temp_df.ipynb ‚ûù PAC\n",
      "Copied and renamed: dictionaries.py ‚ûù PAL\n",
      "Copied and renamed: M12_HDF_03-05_20D0.ipynb ‚ûù M12_PAL_03-05_20D0.ipynb in PAL\n",
      "Copied and renamed: M12_HDF_training.ipynb ‚ûù M12_PAL_training.ipynb in PAL\n",
      "Copied and renamed: M18_HDF_03-05_14D0.ipynb ‚ûù M18_PAL_03-05_14D0.ipynb in PAL\n",
      "Copied and renamed: M18_HDF_training.ipynb ‚ûù M18_PAL_training.ipynb in PAL\n",
      "Copied and renamed: M24_HDF_03-05_08D0.ipynb ‚ûù M24_PAL_03-05_08D0.ipynb in PAL\n",
      "Copied and renamed: M24_HDF_training.ipynb ‚ûù M24_PAL_training.ipynb in PAL\n",
      "Copied and renamed: M36_HDF_03-05_02D0.ipynb ‚ûù M36_PAL_03-05_02D0.ipynb in PAL\n",
      "Copied and renamed: M36_HDF_training.ipynb ‚ûù M36_PAL_training.ipynb in PAL\n",
      "Copied and renamed: M48_HDF_03-05_02D0.ipynb ‚ûù M48_PAL_03-05_02D0.ipynb in PAL\n",
      "Copied and renamed: M48_HDF_training.ipynb ‚ûù M48_PAL_training.ipynb in PAL\n",
      "Copied and renamed: pipeline_cons_temp_df.ipynb ‚ûù PAL\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Base directory for all region folders\n",
    "base_dir = r\"C:\\Users\\Henri\\Documents\\GitHub\\Predi_Conso_Elec_Region\\Predi_Conso_Elec_Region\\notebooks\\Region tests\"\n",
    "\n",
    "# Source region and folder\n",
    "source_region = \"HDF\"\n",
    "source_folder = os.path.join(base_dir, source_region)\n",
    "\n",
    "# List of destination regions\n",
    "target_regions = [\"IDF\", \"NOR\", \"OCC\", \"PAC\", \"PAL\"]\n",
    "\n",
    "# Loop through each destination region\n",
    "for target_region in target_regions:\n",
    "    target_folder = os.path.join(base_dir, target_region)\n",
    "    \n",
    "    # Create the target folder if it doesn't exist\n",
    "    os.makedirs(target_folder, exist_ok=True)\n",
    "\n",
    "    # Copy and rename each file\n",
    "    for filename in os.listdir(source_folder):\n",
    "        if \"HDF\" in filename:\n",
    "            new_filename = filename.replace(\"HDF\", target_region)\n",
    "            source_path = os.path.join(source_folder, filename)\n",
    "            target_path = os.path.join(target_folder, new_filename)\n",
    "            shutil.copy2(source_path, target_path)\n",
    "            print(f\"Copied and renamed: {filename} ‚ûù {new_filename} in {target_region}\")\n",
    "        if \"dictionaries\" in filename:\n",
    "            source_path = os.path.join(source_folder, filename)\n",
    "            target_path = os.path.join(target_folder)\n",
    "            shutil.copy2(source_path, target_path)\n",
    "            print(f\"Copied and renamed: {filename} ‚ûù {target_region}\")\n",
    "        if \"pipeline_cons_temp_df\" in filename:\n",
    "            source_path = os.path.join(source_folder, filename)\n",
    "            target_path = os.path.join(target_folder)\n",
    "            shutil.copy2(source_path, target_path)\n",
    "            print(f\"Copied and renamed: {filename} ‚ûù {target_region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nbformat\n",
      "  Using cached nbformat-5.10.4-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting fastjsonschema>=2.15 (from nbformat)\n",
      "  Using cached fastjsonschema-2.21.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\henri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nbformat) (4.23.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\henri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nbformat) (5.7.2)\n",
      "Requirement already satisfied: traitlets>=5.1 in c:\\users\\henri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nbformat) (5.14.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\henri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jsonschema>=2.6->nbformat) (24.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\henri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jsonschema>=2.6->nbformat) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\henri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jsonschema>=2.6->nbformat) (0.36.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\henri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jsonschema>=2.6->nbformat) (0.22.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\henri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.3.6)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\henri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (308)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in c:\\users\\henri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat) (4.12.2)\n",
      "Using cached nbformat-5.10.4-py3-none-any.whl (78 kB)\n",
      "Using cached fastjsonschema-2.21.1-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: fastjsonschema, nbformat\n",
      "Successfully installed fastjsonschema-2.21.1 nbformat-5.10.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\Henri\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated cell in M12_PAC_training.ipynb\n",
      "Updated cell in M18_PAC_training.ipynb\n",
      "Updated cell in M24_PAC_training.ipynb\n",
      "Updated cell in M36_PAC_training.ipynb\n",
      "Updated cell in M48_PAC_training.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nbformat\n",
    "\n",
    "# Define the base directory with your notebooks\n",
    "notebook_dir = r\"C:\\Users\\Henri\\Documents\\GitHub\\Predi_Conso_Elec_Region\\Predi_Conso_Elec_Region\\notebooks\\Region tests\\PAC\"\n",
    "\n",
    "# Define new region variables\n",
    "new_func_region = \"Provence-Alpes-C√¥te d'Azure\"\n",
    "new_func_caps = \"PAC\"\n",
    "new_func_lwrc = \"pac\"\n",
    "\n",
    "# Define the updated content for the cell\n",
    "updated_code = f'''func_region = \"{new_func_region}\"\n",
    "func_caps = \"{new_func_caps}\"\n",
    "func_lwrc = \"{new_func_lwrc}\"'''\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for file in os.listdir(notebook_dir):\n",
    "    if file.endswith(\"training.ipynb\"):\n",
    "        notebook_path = os.path.join(notebook_dir, file)\n",
    "\n",
    "        # Load the notebook\n",
    "        with open(notebook_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "        # Modify the 4th cell (index 3)\n",
    "        if len(nb.cells) > 3 and nb.cells[3].cell_type == \"code\":\n",
    "            nb.cells[3].source = updated_code\n",
    "            print(f\"Updated cell in {file}\")\n",
    "        else:\n",
    "            print(f\"Skipped {file} (not enough cells or 4th cell not a code cell)\")\n",
    "\n",
    "        # Save the modified notebook\n",
    "        with open(notebook_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            nbformat.write(nb, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Updated notebooks in CVL\n",
      "‚úÖ Updated notebooks in GRE\n",
      "‚úÖ Updated notebooks in HDF\n",
      "‚úÖ Updated notebooks in NOR\n",
      "‚úÖ Updated notebooks in OCC\n",
      "‚úÖ Updated notebooks in PAL\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nbformat\n",
    "import importlib.util\n",
    "\n",
    "# Path to the master directory containing all region folders\n",
    "base_dir = r\"C:\\Users\\Henri\\Documents\\GitHub\\Predi_Conso_Elec_Region\\Predi_Conso_Elec_Region\\notebooks\\Region tests\"\n",
    "\n",
    "# Load the dictionaries.py file (assumed to be in one shared location)\n",
    "dict_path = os.path.join(base_dir, \"NAQ\", \"dictionaries.py\")  # Using one region copy as reference\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\"dictionaries\", dict_path)\n",
    "dictionaries = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(dictionaries)\n",
    "\n",
    "# Regions to skip\n",
    "excluded = {\"ARA\", \"BFC\", \"BRE\", \"IDF\", \"NAQ\", \"PAC\"}\n",
    "\n",
    "# Reverse lookup: abbrev ‚Üí full region name\n",
    "reverse_caps_dict = {v: k for k, v in dictionaries.region_abbr_caps_dict.items()}\n",
    "\n",
    "# Loop over region folders\n",
    "for folder in os.listdir(base_dir):\n",
    "    folder_path = os.path.join(base_dir, folder)\n",
    "\n",
    "    # Skip if not a folder or not a relevant region\n",
    "    if not os.path.isdir(folder_path) or folder in excluded or folder not in reverse_caps_dict:\n",
    "        continue\n",
    "\n",
    "    region_caps = folder  # Folder name like \"NOR\"\n",
    "    region = reverse_caps_dict[region_caps]  # Get full region name\n",
    "    region_lwrc = dictionaries.region_abbr_dict[region]  # lowercase abbreviation\n",
    "    \n",
    "\n",
    "    # Find all notebooks ending with \"training.ipynb\" in the folder\n",
    "    notebook_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(\"training.ipynb\")]\n",
    "\n",
    "    if not notebook_paths:\n",
    "        print(f\"No training notebooks found in {folder}\")\n",
    "        continue\n",
    "\n",
    "    for notebook_path in notebook_paths:\n",
    "        # Modify each notebook in the list\n",
    "        nb = nbformat.read(notebook_path, as_version=4)\n",
    "\n",
    "    for cell in nb.cells:\n",
    "        if cell.cell_type == 'code' and 'func_region' in cell.source:\n",
    "            cell.source = f'''func_region = \"{region}\"\n",
    "func_caps = \"{folder}\"\n",
    "func_lwrc = \"{region_lwrc}\"'''\n",
    "            break  # Only update the first matching code cell\n",
    "\n",
    "    with open(notebook_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        nbformat.write(nb, f)\n",
    "   \n",
    "    print(f\"‚úÖ Updated notebooks in {folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Migrating MLFlow models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying model xgb_model_ara_m36 from region ARA to centralized models folder.\n",
      "Copying model xgb_model_ara_m12 from region ARA to centralized models folder.\n",
      "Copying model xgb_model_ara_m48 from region ARA to centralized models folder.\n",
      "Copying model xgb_model_ara_m18 from region ARA to centralized models folder.\n",
      "Model xgb_model_ara_m12 already exists in centralized models folder. Skipping.\n",
      "Copying model xgb_model_ara_m24 from region ARA to centralized models folder.\n",
      "Copying model xgb_model_bfc_m18 from region BFC to centralized models folder.\n",
      "Copying model xgb_model_bfc_m48 from region BFC to centralized models folder.\n",
      "Copying model xgb_model_bfc_m12 from region BFC to centralized models folder.\n",
      "Copying model xgb_model_bfc_m24 from region BFC to centralized models folder.\n",
      "Copying model xgb_model_bfc_m36 from region BFC to centralized models folder.\n",
      "Copying model xgb_model_bre_m36 from region BRE to centralized models folder.\n",
      "Copying model xgb_model_bre_m12 from region BRE to centralized models folder.\n",
      "Copying model xgb_model_bre_m48 from region BRE to centralized models folder.\n",
      "Copying model xgb_model_bre_m24 from region BRE to centralized models folder.\n",
      "Copying model xgb_model_cvl_m18 from region CVL to centralized models folder.\n",
      "Copying model xgb_model_cvl_m48 from region CVL to centralized models folder.\n",
      "Copying model xgb_model_cvl_m24 from region CVL to centralized models folder.\n",
      "Copying model xgb_model_cvl_m36 from region CVL to centralized models folder.\n",
      "Copying model xgb_model_gre_m48 from region GRE to centralized models folder.\n",
      "Copying model xgb_model_gre_m12 from region GRE to centralized models folder.\n",
      "Copying model xgb_model_gre_m36 from region GRE to centralized models folder.\n",
      "Copying model xgb_model_gre_m18 from region GRE to centralized models folder.\n",
      "Copying model xgb_model_gre_m24 from region GRE to centralized models folder.\n",
      "Copying model xgb_model_hdf_m36 from region HDF to centralized models folder.\n",
      "Copying model xgb_model_hdf_m24 from region HDF to centralized models folder.\n",
      "Copying model xgb_model_hdf_m48 from region HDF to centralized models folder.\n",
      "Copying model xgb_model_hdf_m12 from region HDF to centralized models folder.\n",
      "Copying model xgb_model_hdf_m18 from region HDF to centralized models folder.\n",
      "Copying model xgb_model_idf_m12 from region IDF to centralized models folder.\n",
      "Copying model xgb_model_idf_m36 from region IDF to centralized models folder.\n",
      "Model xgb_model_idf_m12 already exists in centralized models folder. Skipping.\n",
      "Copying model xgb_model_idf_m24 from region IDF to centralized models folder.\n",
      "Copying model xgb_model_idf_m48 from region IDF to centralized models folder.\n",
      "Copying model xgb_model_idf_m18 from region IDF to centralized models folder.\n",
      "Copying model xgb_model_occ_m36 from region OCC to centralized models folder.\n",
      "Copying model xgb_model_occ_m48 from region OCC to centralized models folder.\n",
      "Copying model xgb_model_occ_m12 from region OCC to centralized models folder.\n",
      "Copying model xgb_model_occ_m18 from region OCC to centralized models folder.\n",
      "Copying model xgb_model_occ_m24 from region OCC to centralized models folder.\n",
      "Copying model xgb_model_pal_m48 from region PAL to centralized models folder.\n",
      "Copying model xgb_model_pal_m12 from region PAL to centralized models folder.\n",
      "Copying model xgb_model_pal_m36 from region PAL to centralized models folder.\n",
      "Copying model xgb_model_pal_m18 from region PAL to centralized models folder.\n",
      "‚úÖ Model migration complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define base directories\n",
    "base_notebooks_dir = os.path.join(\"C:/Users/Henri/Documents/GitHub/Predi_Conso_Elec_Region/Predi_Conso_Elec_Region/notebooks/Region_tests\")\n",
    "central_model_dir = os.path.join(\"C:/Users/Henri/Documents/GitHub/Predi_Conso_Elec_Region/Predi_Conso_Elec_Region/models\")\n",
    "\n",
    "# Ensure target directory exists\n",
    "os.makedirs(central_model_dir, exist_ok=True)\n",
    "\n",
    "# Go through each region folder\n",
    "for region in os.listdir(base_notebooks_dir):\n",
    "    region_path = os.path.join(base_notebooks_dir, region, \"mlruns\", \"0\")\n",
    "\n",
    "    if not os.path.exists(region_path):\n",
    "        continue\n",
    "\n",
    "    # Loop through run folders\n",
    "    for run_id in os.listdir(region_path):\n",
    "        run_path = os.path.join(region_path, run_id, \"artifacts\")\n",
    "        if not os.path.exists(run_path):\n",
    "            continue\n",
    "\n",
    "        # Look for any model artifact directories (e.g., xgb_model_bfc_m18)\n",
    "        for artifact in os.listdir(run_path):\n",
    "            artifact_path = os.path.join(run_path, artifact)\n",
    "\n",
    "            if os.path.isdir(artifact_path):\n",
    "                # Construct target path\n",
    "                target_folder = os.path.join(central_model_dir, artifact + \"_v1\")\n",
    "                if not os.path.exists(target_folder):\n",
    "                    print(f\"Copying model {artifact} from region {region} to centralized models folder.\")\n",
    "                    shutil.copytree(artifact_path, target_folder)\n",
    "                else:\n",
    "                    print(f\"Model {artifact} already exists in centralized models folder. Skipping.\")\n",
    "\n",
    "print(\"‚úÖ Model migration complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing curly apostrophe (right single quotation mark, unicode 8217) to straight apostrophe (unicode 39) in merged_full.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the original file\n",
    "df = pd.read_csv(r\"data\\merged_full.csv\")\n",
    "\n",
    "# Replace curly apostrophes with straight ones in the 'R√©gion' column\n",
    "df[\"R√©gion\"] = df[\"R√©gion\"].str.replace(\"‚Äô\", \"'\")\n",
    "\n",
    "# Overwrite the original file\n",
    "df.to_csv(r\"data\\merged_full.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same idea, different approach : enforcing that all characters are in standard form when reading CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "# Load the CSV\n",
    "df_original = pd.read_csv(\"merged_full.csv\")\n",
    "\n",
    "# Normalize all entries in the 'R√©gion' column\n",
    "df_original[\"R√©gion\"] = df_original[\"R√©gion\"].apply(\n",
    "    lambda x: unicodedata.normalize(\"NFKC\", x) if isinstance(x, str) else x\n",
    ")\n",
    "\n",
    "# Check the output again\n",
    "for r in df_original[\"R√©gion\"].unique():\n",
    "    print(repr(r))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifically migrating Version 3 of M12 IDF to centralized models folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model xgb_model_idf_m12 (v3) successfully copied to centralized models folder.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Inputs\n",
    "run_id = \"d3d5b6de70044709bb80f031325be8a8\"\n",
    "model_name = \"xgb_model_idf_m12\"\n",
    "central_model_dir = r\"C:\\Users\\Henri\\Documents\\GitHub\\Predi_Conso_Elec_Region\\Predi_Conso_Elec_Region\\models\"\n",
    "\n",
    "# Construct full source path to artifact\n",
    "source_model_path = os.path.join(\n",
    "    r\"C:\\Users\\Henri\\Documents\\GitHub\\Predi_Conso_Elec_Region\\Predi_Conso_Elec_Region\\notebooks\\Region tests\\IDF\\mlruns\\0\",\n",
    "    run_id,\n",
    "    \"artifacts\",\n",
    "    model_name\n",
    ")\n",
    "\n",
    "# Define destination path in centralized folder\n",
    "target_model_path = os.path.join(central_model_dir, model_name + \"_v3\")\n",
    "\n",
    "# Copy if it doesn't already exist\n",
    "if not os.path.exists(target_model_path):\n",
    "    shutil.copytree(source_model_path, target_model_path)\n",
    "    print(f\"‚úÖ Model {model_name} (v3) successfully copied to centralized models folder.\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Model {model_name} (v3) already exists. Skipping.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copying other models (BRE, CVL, PRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model xgb_model_bre_m18 (v1) successfully copied to centralized models folder.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Inputs\n",
    "run_id = \"90dc93e328c4410d9d5c0bd11224d552\"\n",
    "model_name = \"xgb_model_bre_m18\"\n",
    "central_model_dir = r\"C:\\Users\\Henri\\Documents\\GitHub\\Predi_Conso_Elec_Region\\Predi_Conso_Elec_Region\\models\"\n",
    "\n",
    "# Construct full source path to artifact\n",
    "source_model_path = os.path.join(\n",
    "    r\"C:\\Users\\Henri\\Documents\\GitHub\\Predi_Conso_Elec_Region\\Predi_Conso_Elec_Region\\notebooks\\Region tests\\BRE\\mlruns\\0\",\n",
    "    run_id,\n",
    "    \"artifacts\",\n",
    "    model_name\n",
    ")\n",
    "\n",
    "# Define destination path in centralized folder\n",
    "target_model_path = os.path.join(central_model_dir, model_name + \"_v1\")\n",
    "\n",
    "# Copy if it doesn't already exist\n",
    "if not os.path.exists(target_model_path):\n",
    "    shutil.copytree(source_model_path, target_model_path)\n",
    "    print(f\"‚úÖ Model {model_name} (v1) successfully copied to centralized models folder.\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Model {model_name} (v1) already exists. Skipping.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model xgb_model_cvl_m12 (v1) successfully copied to centralized models folder.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Inputs\n",
    "run_id = \"1cc9f780732a49bd804e85c2f863e8f3\"\n",
    "model_name = \"xgb_model_cvl_m12\"\n",
    "central_model_dir = r\"C:\\Users\\Henri\\Documents\\GitHub\\Predi_Conso_Elec_Region\\Predi_Conso_Elec_Region\\models\"\n",
    "\n",
    "# Construct full source path to artifact\n",
    "source_model_path = os.path.join(\n",
    "    r\"C:\\Users\\Henri\\Documents\\GitHub\\Predi_Conso_Elec_Region\\Predi_Conso_Elec_Region\\notebooks\\Region tests\\CVL\\mlruns\\0\",\n",
    "    run_id,\n",
    "    \"artifacts\",\n",
    "    model_name\n",
    ")\n",
    "\n",
    "# Define destination path in centralized folder\n",
    "target_model_path = os.path.join(central_model_dir, model_name + \"_v1\")\n",
    "\n",
    "# Copy if it doesn't already exist\n",
    "if not os.path.exists(target_model_path):\n",
    "    shutil.copytree(source_model_path, target_model_path)\n",
    "    print(f\"‚úÖ Model {model_name} (v1) successfully copied to centralized models folder.\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Model {model_name} (v1) already exists. Skipping.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model xgb_model_pal_m24 (v1) successfully copied to centralized models folder.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Inputs\n",
    "run_id = \"12845fc201e84ddab8a8707d84866e6f\"\n",
    "model_name = \"xgb_model_pal_m24\"\n",
    "central_model_dir = r\"C:\\Users\\Henri\\Documents\\GitHub\\Predi_Conso_Elec_Region\\Predi_Conso_Elec_Region\\models\"\n",
    "\n",
    "# Construct full source path to artifact\n",
    "source_model_path = os.path.join(\n",
    "    r\"C:\\Users\\Henri\\Documents\\GitHub\\Predi_Conso_Elec_Region\\Predi_Conso_Elec_Region\\notebooks\\Region tests\\PAL\\mlruns\\0\",\n",
    "    run_id,\n",
    "    \"artifacts\",\n",
    "    model_name\n",
    ")\n",
    "\n",
    "# Define destination path in centralized folder\n",
    "target_model_path = os.path.join(central_model_dir, model_name + \"_v1\")\n",
    "\n",
    "# Copy if it doesn't already exist\n",
    "if not os.path.exists(target_model_path):\n",
    "    shutil.copytree(source_model_path, target_model_path)\n",
    "    print(f\"‚úÖ Model {model_name} (v1) successfully copied to centralized models folder.\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Model {model_name} (v1) already exists. Skipping.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfering over all models for NAQ and PACA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Copying model xgb_model_naq_m18 from region NAQ to centralized folder.\n",
      "üì¶ Copying model xgb_model_naq_m48 from region NAQ to centralized folder.\n",
      "üì¶ Copying model xgb_model_naq_m36 from region NAQ to centralized folder.\n",
      "üì¶ Copying model xgb_model_naq_m24 from region NAQ to centralized folder.\n",
      "üì¶ Copying model xgb_model_naq_m12 from region NAQ to centralized folder.\n",
      "üì¶ Copying model xgb_model_pac_m36 from region PAC to centralized folder.\n",
      "üì¶ Copying model xgb_model_pac_m24 from region PAC to centralized folder.\n",
      "üì¶ Copying model xgb_model_pac_m18 from region PAC to centralized folder.\n",
      "üì¶ Copying model xgb_model_pac_m12 from region PAC to centralized folder.\n",
      "üì¶ Copying model xgb_model_pac_m48 from region PAC to centralized folder.\n",
      "üèÅ Migration complete for NAQ and PAC.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Base directories\n",
    "base_notebooks_dir = r\"C:\\Users\\Henri\\Documents\\GitHub\\Predi_Conso_Elec_Region\\Predi_Conso_Elec_Region\\notebooks\\Region_tests\"\n",
    "central_model_dir = r\"C:\\Users\\Henri\\Documents\\GitHub\\Predi_Conso_Elec_Region\\Predi_Conso_Elec_Region\\models\"\n",
    "\n",
    "# Only these regions\n",
    "target_regions = [\"NAQ\", \"PAC\"]\n",
    "\n",
    "# Ensure target directory exists\n",
    "os.makedirs(central_model_dir, exist_ok=True)\n",
    "\n",
    "# Loop over only NAQ and PAC\n",
    "for region in target_regions:\n",
    "    region_path = os.path.join(base_notebooks_dir, region, \"mlruns\", \"0\")\n",
    "\n",
    "    if not os.path.exists(region_path):\n",
    "        print(f\"‚ö†Ô∏è Region path not found: {region_path}\")\n",
    "        continue\n",
    "\n",
    "    for run_id in os.listdir(region_path):\n",
    "        run_path = os.path.join(region_path, run_id, \"artifacts\")\n",
    "\n",
    "        if not os.path.exists(run_path):\n",
    "            continue\n",
    "\n",
    "        for artifact in os.listdir(run_path):\n",
    "            artifact_path = os.path.join(run_path, artifact)\n",
    "\n",
    "            if os.path.isdir(artifact_path):\n",
    "                target_folder = os.path.join(central_model_dir, artifact + \"_v1\")\n",
    "\n",
    "                if not os.path.exists(target_folder):\n",
    "                    print(f\"üì¶ Copying model {artifact} from region {region} to centralized folder.\")\n",
    "                    shutil.copytree(artifact_path, target_folder)\n",
    "                else:\n",
    "                    print(f\"‚úÖ Model {artifact} already exists. Skipping.\")\n",
    "\n",
    "print(\"üèÅ Migration complete for NAQ and PAC.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Region Prediction Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dictionaries import region_abbr_caps_dict\n",
    "\n",
    "base_prediction_dir = \"Predictions\"\n",
    "\n",
    "# Create the base Predictions directory if it doesn't exist\n",
    "os.makedirs(base_prediction_dir, exist_ok=True)\n",
    "\n",
    "# Create subfolders for each region abbreviation\n",
    "for region_abbr in region_abbr_caps_dict.values():\n",
    "    region_path = os.path.join(base_prediction_dir, region_abbr)\n",
    "    os.makedirs(region_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
